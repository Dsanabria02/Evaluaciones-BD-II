<image
  src="logo_tec.jpg"
  alt=""
  caption="tttt">


 <h1>Instituto Tecnológico de Costa Rica</h1> 


 <font size="3"> Campus Tecnológico Central Cartago </font> 
 
 <font size="3"> Escuela de Ingeniería en Computación </font>

 <b><font size="3"> Bases de Datos II  </font></b>

<font size="3"> IC-4302 </font>

<b><font size="3"> Resumen 3 </font></b>

<font size="3"> Fecha de entrega: 21/03/2023 </font>

<b><font size="3"> I-Semestre 2023  </font></b>

<font size="3"> Profesor:   </font>
 
<b><font size="3"> Nereo Campos Araya.</font></b>

<font size="3"> Elaborado por:</font>

<b><font size="3"> Diana Sanabria Calvo 2021436548.</font>

<br>

<br>

<br> 

<br>

<br>

<font face="Comic Sans MS,arial"><center><font size="4"><h2><FONT COLOR="#48C9B0 ">--------Apache Spark--------</h2></font></center>

<html>
        <p><font size="4"><FONT SIZE=6 COLOR="#48C9B0 ">P</FONT></FONT>odemos ver que la generación diaria de datos aumenta exponencialmente, también lo hace la necesidad de un procesamiento efectivo de esos datos. Los sistemas de procesamiento de datos tradicionales, como los basados en <font size="4"><FONT SIZE=4 COLOR="#48C9B0 ">bases de datos relacionales </FONT></FONT>, tampoco son escalables para grandes volúmenes de datos. También se observa que el artículo argumenta que los métodos de procesamiento de datos convencionales son insuficientes para el procesamiento de datos en tiempo real y como resultado, no pueden ofrecer resultados en tiempo real. <p><font size="4"><FONT SIZE=6 COLOR="#48C9B0 ">P</FONT></FONT>ara superar estas dificultades y ofrecer un método escalable y efectivo para procesar <font size="4"><FONT SIZE=4 COLOR="#48C9B0 ">grandes volúmenes de datos </FONT></FONT>, se necesita un enfoque novedoso para el procesamiento de grandes conjuntos de datos. Debido a la arquitectura de hardware inherente, la complejidad de los algoritmos empleados y la falta de flexibilidad en el procesamiento de datos en tiempo real, los sistemas de procesamiento de datos tradicionales tienen limitaciones en términos de escalabilidad y rendimiento. Para procesar conjuntos de datos masivos en clústeres de computadoras, Apache Spark es un sistema de <font size="4"><FONT SIZE=4 COLOR="#48C9B0 ">procesamiento de datos unificados </FONT></FONT> que ofrece una interfaz de programación sencilla y consistente. 
        <p><font size="4"><FONT SIZE=6 COLOR="#48C9B0 ">A</FONT></FONT>demás, Apache Spark ofrece una biblioteca de operaciones distribuidas para el procesamiento de datos en memoria, lo que acelera el procesamiento de datos al <font size="4"><FONT SIZE=4 COLOR="#48C9B0 ">almacenarlos en la memoria del clúster en lugar de en el disco.</FONT></FONT> A lo largo del artículo se afirma que la arquitectura Apache Spark consta de una serie de partes, incluido Spark Core, que proporciona las capacidades informáticas y de escalabilidad fundamentales del sistema; los módulos <font size="4"><FONT SIZE=4 COLOR="#48C9B0 ">Spark SQL y DataFrames </FONT></FONT>, que brindan capacidades para procesar datos estructurados y los <font size="4"><FONT SIZE=4 COLOR="#48C9B0 ">módulos Spark Streaming y Structured Streaming </FONT></FONT>, que ofrecen capacidades de procesamiento de datos en tiempo real. <p><font size="4"><FONT SIZE=6 COLOR="#48C9B0 ">E</FONT></FONT>l modelo de programación de Apache Spark se basa en la idea de los <font size="4"><FONT SIZE=4 COLOR="#48C9B0 ">RDD</FONT></FONT>(conjuntos de datos distribuidos resistentes), que son colecciones inmutables de objetos que se pueden distribuir en un grupo de computadoras y procesar en paralelo y los <font size="4"><FONT SIZE=4 COLOR="#48C9B0 ">RDD</FONT></FONT> permiten el <font size="4"><FONT SIZE=4 COLOR="#48C9B0 ">procesamiento de datos en paralelo</FONT></FONT>, lo que mejora el rendimiento del sistema. Para manejar conjuntos de datos de cualquier tamaño, desde <font size="4"><FONT SIZE=4 COLOR="#48C9B0 ">gigabytes hasta petabytes</FONT></FONT>, Apache Spark está diseñado para ser altamente escalable. Esto es posible mediante la división de datos y cálculos entre grupos de computadoras, además de tener la opción de agregar o eliminar nodos sobre la marcha para adaptarse a las cambiantes demandas de procesamiento de datos. Debido al uso de <font size="4"><FONT SIZE=4 COLOR="#48C9B0 ">RAM</FONT></FONT> y la optimización de las operaciones de procesamiento de datos en la memoria, Apache Spark es más rápido que muchos otros sistemas de procesamiento de datos.<p><font size="4"><FONT SIZE=6 COLOR="#48C9B0 ">L</FONT></FONT>a abstracción <font size="4"><FONT SIZE=4 COLOR="#48C9B0 ">RDD (Resilient Distributed Datasets)</FONT></FONT> sirve como base para el modelo de programación de Spark, que permite el procesamiento paralelo de colecciones inmutables de objetos distribuidos en un grupo de computadoras. Se explica cómo las transformaciones y las acciones son los dos tipos de operaciones de <font size="4"><FONT SIZE=4 COLOR="#48C9B0 ">RDD</FONT></FONT> y como devuelven un resultado al agente de usuario o escriben datos en un almacenamiento externo, como contar o escribir, mientras que las transformaciones son operaciones que crean un nuevo RDD a partir de uno existente, como filtrar o mapear.

 </p></font>
	</div>
</font>
</body>
</html>
<br>


